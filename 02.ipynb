{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심화 과제 1 - Solve Computer vision task using MLP \n",
    "1) 과제 개요\n",
    "- 본 과제는 MLP 모델을 이용하여 Computer vision task를 해결을 목표로 하는 과제 입니다.\n",
    "- 본 과제는 다음과 같은 단계로 진행됩니다.\n",
    "    0. debug image data 만들기\n",
    "    1. MLP 모델 설계\n",
    "    2. 데이터셋 준비\n",
    "    3. 모델 학습\n",
    "    4. 모델 평가\n",
    "    5. 모델 저장 및 불러오기\n",
    "    6. 새로운 이미지에 대한 추론\n",
    "2) 과제 출제 목적 및 배경\n",
    "- 본 과제는 목표하는 모델을 설계하고 학습하는 과정을 통해 딥러닝 모델을 설계하고 학습하는 방법을 익히기 위한 것입니다.\n",
    "- 목표한 모델을 잘 만들었는지 확인하는 디버깅 능력 또한 함양하기 위한 것입니다.\n",
    "- 논문에서 제시한 모델을 구현하는 능력을 함양하기 위한 것입니다.\n",
    "\n",
    "3) 과제 수행 방법\n",
    "\n",
    "주어진 코드에서 [START CODE]과 [END CODE] 주석 사이의 셀을 완성하여 과제를 수행합니다. 해당 셀에서 목표하는 기능을 달성할 수 있도록 코드를 작성해 주세요.\n",
    "\n",
    "4) 구현 딥러닝 모델\n",
    "- MLP-Mixer\n",
    "- MLP-Mixer는 MLP 모델을 이용하여 Computer vision task를 해결하기 위한 모델입니다.\n",
    "- MLP-Mixer는 다음과 같은 구조로 이루어져 있습니다.\n",
    "    - MLP-Mixer는 Token-mixing MLP과 Channel-mixing MLP로 구성되어 있습니다.\n",
    "    - Token-mixing MLP는 이미지의 각 픽셀에 대한 정보를 섞어주는 역할을 합니다.\n",
    "    - Channel-mixing MLP는 이미지의 각 채널에 대한 정보를 섞어주는 역할을 합니다.\n",
    "\n",
    "![image.png](./assets/mlp_mixer.png)\n",
    "\n",
    "5) 사용 데이터셋\n",
    "\n",
    "- 본 과제에서는 CIFAR-10 데이터셋을 사용합니다.\n",
    "\n",
    "![image-2.png](./assets/cifar_10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: list, output_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(hidden_size)):\n",
    "            if i == 0:\n",
    "                self.hidden_layers.append(self.hidden_block(input_size, hidden_size[i]))\n",
    "            else:\n",
    "                self.hidden_layers.append(\n",
    "                    self.hidden_block(hidden_size[i - 1], hidden_size[i])\n",
    "                )\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size[-1], output_size)\n",
    "\n",
    "    def hidden_block(self, input_size: int, hidden_size: int):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import cifar10 dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "cifar10_test = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# [CORRECT CODE]\n",
    "import torch.optim as optim\n",
    "\n",
    "# [START CODE]\n",
    "# model = MLPMixer()\n",
    "model = NormalMLP(32 * 32 * 3, [64, 32, 16], 10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# [END CODE]\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "        label = data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            image = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "\n",
    "def do_train_test(\n",
    "    model, train_loader, test_loader, criterion, optimizer, device, num_epochs\n",
    "):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\n",
    "            f\"epoch: {epoch + 1}, train loss: {train_loss:.4f}, test loss: {test_loss:.4f}, test acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    plt.plot(train_loss_list, label=\"train loss\")\n",
    "    plt.plot(test_loss_list, label=\"test loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(test_acc_list, label=\"test acc\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"acc\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train_test(model, train_loader, test_loader, criterion, optimizer, \"cuda\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare debug image data\n",
    "\n",
    "- 본 과제에서는 MLP-Mixer 모델을 구현하고 테스트하기 위해 특이한 debug image를 생성하도록 하겠습니다.\n",
    "\n",
    "- 제안 함수\n",
    "    - `torch.full` : 주어진 크기와 값을 가지는 tensor를 생성합니다. args로는 `size`, `fill_value`, `dtype`, `device`, `requires_grad가` 있습니다.\n",
    "    - `torch.cat` : 주어진 dim으로 tensor를 이어붙입니다. args로는 `tensors`, `dim이` 있습니다. (`tensors`는 이어붙일 tensor들의 list입니다.)\n",
    "    - `torch.randperm` : 주어진 범위 내에서 무작위로 순열을 생성합니다. args로는 `n`이 있습니다.\n",
    "    - `unsqueeze` : 주어진 위치에 차원을 추가합니다.\n",
    "\n",
    "위에 제안된 함수를 활용하여 다음과 같은 이미지를 생성해보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [START CODE]\n",
    "# # 각 패치를 생성합니다.\n",
    "# patches = None\n",
    "\n",
    "# # 패치를 3x3 그리드에 배치합니다.\n",
    "# # 3개 패치를 하나의 행으로 결합합니다.\n",
    "# rows = None\n",
    "# # 3개 행을 하나의 이미지로 결합합니다.\n",
    "# debug_image = None\n",
    "\n",
    "# # 3ch 이미지로 확장합니다.\n",
    "# debug_image = None\n",
    "\n",
    "# # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "# [START CODE]\n",
    "# 각 패치를 생성합니다.\n",
    "patches = [torch.full((3, 3), i) for i in range(9)]\n",
    "\n",
    "# 패치를 3x3 그리드에 배치합니다.\n",
    "rows = [torch.cat(patches[i * 3 : (i + 1) * 3], dim=1) for i in range(3)]\n",
    "debug_image = torch.cat(rows, dim=0)\n",
    "\n",
    "# 3ch 이미지로 확장합니다.\n",
    "debug_image = debug_image.unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "# [END CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check debug image data\n",
    "- 생성된 이미지 데이터를 시각화하여 확인해보세요.\n",
    "\n",
    "이미지 데이터의 값은 [0, 255]이기 때문에 0과 8 사이의 값으로 시각화 시에는 값의 차이가 크지 않아 잘만들어졌는지 확인하기 어려워 [0, 1]로 normalize 이후 값을 확인하도록 하겠습니다.\n",
    "\n",
    "- 제안 함수\n",
    "    - `torch.tensor.permute` : 주어진 차원을 순서대로 바꿉니다.\n",
    "    - `plt.imshow` : 이미지를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [START CODE]\n",
    "# # channels first -> channels last\n",
    "# channels_last_image = None\n",
    "# # Normalize the image\n",
    "# normalized_image = None\n",
    "# # [END CODE]\n",
    "# # Show the image\n",
    "# plt.imshow(normalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x30b28fca0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [CORRECT CODE]\n",
    "# [START CODE]\n",
    "# channels first -> channels last\n",
    "channels_last_image = debug_image.permute(1, 2, 0)\n",
    "# Normalize the image\n",
    "normalized_image = channels_last_image / 8\n",
    "# Show the image\n",
    "plt.imshow(normalized_image)\n",
    "# [END CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "![image](./assets/debug_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만든 이미지를 배치 차원을 추가하여 4차원으로 만들고 이를 모델에 적용할 수 있게 하기 위해 float로 변환해주세요.\n",
    "- 제안함수\n",
    "    - `unsqueeze` : 주어진 위치에 차원을 추가합니다.\n",
    "    - `repeat` : 주어진 차원을 반복합니다.\n",
    "    - `float` : tensor의 dtype을 float로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [CORRECT CODE]\n",
    "# batch_size = 32\n",
    "# # [START CODE]\n",
    "# batch_debug_image = None\n",
    "# # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "batch_size = 32\n",
    "# [START CODE]\n",
    "batch_debug_image = debug_image.unsqueeze(0).repeat(batch_size, 1, 1, 1).float()\n",
    "# [END CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP-Mixer\n",
    "이제 본격적으로 MLP-Mixer 모델을 구현해보겠습니다.\n",
    "\n",
    "MLP-Mixer의 가장 첫 부분은 이미지를 각 패치별로 나누는 부분입니다. 이를 위해 이미지를 패치로 나누는 함수를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch splitting\n",
    "\n",
    "MLP-Mixer의 가장 첫 부분은 이미지를 각 패치별로 나누는 부분입니다. 이를 위해 이미지를 패치로 나누는 함수를 구현해보겠습니다.\n",
    "\n",
    "- 제안 함수\n",
    "    - `torch.nn.unfold` : 주어진 kernel size로 이미지를 나눕니다. args로는 `kernel_size`, `stride`가 있습니다.\n",
    "    - `einops.layer.torch.Rearrange` : 주어진 차원을 순서대로 바꿉니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "class PatchSplitter(nn.Module):\n",
    "    def __init__(self, patch_size, image_size, batch_size):\n",
    "        super().__init__()\n",
    "        # [START CODE]\n",
    "        self.split = Rearrange(\n",
    "            \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_size, p2=patch_size\n",
    "        )\n",
    "        # [END CODE]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [START CODE]\n",
    "        return self.split(x)\n",
    "        # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatchSplitter(nn.Module):\n",
    "#     def __init__(self, patch_size, image_size, batch_size):\n",
    "#         super().__init__()\n",
    "#         # [START CODE]\n",
    "#         self.split = None\n",
    "#         # [END CODE]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # [START CODE]\n",
    "\n",
    "#         return None\n",
    "#         # [END CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만든 PatchSplitter를 이용하여 이미지를 패치로 나누고 이를 시각화하여 확인해보세요. \n",
    "\n",
    "앞서 저희가 만든 `batch_debug_image`의 patch_size는 3이었으니 이를 이용하여 `patch_size`를 `3`으로 설정해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "# [START CODE]\n",
    "patch_size = 3\n",
    "image_size = debug_image.shape[2]\n",
    "batch_size = batch_debug_image.shape[0]\n",
    "\n",
    "ps = PatchSplitter(patch_size=patch_size, image_size=image_size, batch_size=batch_size)\n",
    "\n",
    "ps_output = ps(batch_debug_image)\n",
    "# [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [CORRECT CODE]\n",
    "# # [START CODE]\n",
    "# patch_size = 3\n",
    "# image_size = debug_image.shape[2]\n",
    "# batch_size = batch_debug_image.shape[0]\n",
    "\n",
    "# ps = PatchSplitter(patch_size=patch_size, image_size=image_size, batch_size=batch_size)\n",
    "\n",
    "# ps_output = ps(batch_debug_image)\n",
    "# # [END CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "- Expected shape of output : `(batch_size, num_patches, patch_size * patch_size * num_channels)`\n",
    "    - split된 patch들은 embedding을 위해 flatten되어 `patch_size * patch_size * num_channels`로 변환되어야 합니다.\n",
    "- Expected value of output : [0, 8]\n",
    "    - 패치 별로 값이 0부터 8까지 반환되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of the output\n",
    "ps_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4.])\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5.])\n",
      "tensor([6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6.])\n",
      "tensor([7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
      "        7., 7., 7., 7., 7., 7., 7., 7., 7.])\n",
      "tensor([8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.,\n",
      "        8., 8., 8., 8., 8., 8., 8., 8., 8.])\n"
     ]
    }
   ],
   "source": [
    "# check the output each patch\n",
    "for i in range(9):\n",
    "    print(ps_output[0, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch Embedding\n",
    "이제 PatchSplitter를 이용하여 나눈 패치들을 임베딩하는 부분을 구현해보겠습니다.\n",
    "\n",
    "- 제안 함수\n",
    "    - `torch.nn.Linear` : 주어진 input을 linear transformation합니다. args로는 `in_features`, `out_features`가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "class PatchEmbLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # [START CODE]\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        # [END CODE]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [START CODE]\n",
    "        return self.linear(x)\n",
    "        # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatchEmbLayer(nn.Module):\n",
    "#     def __init__(self, in_features, out_features):\n",
    "#         super().__init__()\n",
    "#         # [START CODE]\n",
    "#         self.linear = None\n",
    "#         # [END CODE]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # [START CODE]\n",
    "#         return None\n",
    "#         # [END CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "분할된 패치들을 임베딩한 결과를 확인을 위해 `weight와 bias를 2`로 설정하고 결과를 확인해보세요.\n",
    "\n",
    "- Expected shape of output : `(batch_size, num_patches, embed_dim)`\n",
    "    - split된 patch들은 embedding을 통해 `embed_dim`차원으로 변환되어야 합니다.\n",
    "- Expected value of output : [2, 56, 110, ..., 434]\n",
    "    - patch_0의 벡터 경우 깂들이 0으로 구성되어 있고 이를 2로 구성되어 있는 weight와 matrix multiplication한 이후 bias를 더한 값이 나와야 합니다.\n",
    "    - patch_0 : 2\n",
    "    - patch_1 : 56\n",
    "    - patch_2 : 110\n",
    "    - ...\n",
    "    - patch_8 : 434\n",
    "\n",
    "- 제안 함수\n",
    "    - `parameters` : 모델의 parameter를 반환합니다.\n",
    "    - `fill` : tensor의 값을 주어진 값으로 채웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "output_features = 32\n",
    "# [START CODE]\n",
    "in_features = ps_output.shape[-1]\n",
    "\n",
    "pe = PatchEmbLayer(in_features=in_features, out_features=output_features)\n",
    "for pe_parameter in pe.parameters():\n",
    "    pe_parameter.data.fill_(2)\n",
    "pe_output = pe(ps_output)\n",
    "# [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_features = 32\n",
    "# # [START CODE]\n",
    "# in_features = None\n",
    "\n",
    "# pe = PatchEmbLayer(in_features=in_features, out_features=output_features)\n",
    "\n",
    "# # change the weight of the linear layer to 2\n",
    "# # something to do with the weight initialization\n",
    "\n",
    "# pe_output = pe(ps_output)\n",
    "# # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of the output\n",
    "# b n d (batch, number of patches, embedding dimension)\n",
    "pe_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This patch is 0th patch input\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.])\n",
      "This patch is 0th patch output\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This patch is 1th patch input\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "This patch is 1th patch output\n",
      "tensor([56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56.,\n",
      "        56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56., 56.,\n",
      "        56., 56., 56., 56.], grad_fn=<SelectBackward0>)\n",
      "This patch is 2th patch input\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "This patch is 2th patch output\n",
      "tensor([110., 110., 110., 110., 110., 110., 110., 110., 110., 110., 110., 110.,\n",
      "        110., 110., 110., 110., 110., 110., 110., 110., 110., 110., 110., 110.,\n",
      "        110., 110., 110., 110., 110., 110., 110., 110.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This patch is 3th patch input\n",
      "tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "This patch is 3th patch output\n",
      "tensor([164., 164., 164., 164., 164., 164., 164., 164., 164., 164., 164., 164.,\n",
      "        164., 164., 164., 164., 164., 164., 164., 164., 164., 164., 164., 164.,\n",
      "        164., 164., 164., 164., 164., 164., 164., 164.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This patch is 4th patch input\n",
      "tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4.])\n",
      "This patch is 4th patch output\n",
      "tensor([218., 218., 218., 218., 218., 218., 218., 218., 218., 218., 218., 218.,\n",
      "        218., 218., 218., 218., 218., 218., 218., 218., 218., 218., 218., 218.,\n",
      "        218., 218., 218., 218., 218., 218., 218., 218.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This patch is 5th patch input\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5.])\n",
      "This patch is 5th patch output\n",
      "tensor([272., 272., 272., 272., 272., 272., 272., 272., 272., 272., 272., 272.,\n",
      "        272., 272., 272., 272., 272., 272., 272., 272., 272., 272., 272., 272.,\n",
      "        272., 272., 272., 272., 272., 272., 272., 272.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This patch is 6th patch input\n",
      "tensor([6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6.])\n",
      "This patch is 6th patch output\n",
      "tensor([326., 326., 326., 326., 326., 326., 326., 326., 326., 326., 326., 326.,\n",
      "        326., 326., 326., 326., 326., 326., 326., 326., 326., 326., 326., 326.,\n",
      "        326., 326., 326., 326., 326., 326., 326., 326.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This patch is 7th patch input\n",
      "tensor([7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
      "        7., 7., 7., 7., 7., 7., 7., 7., 7.])\n",
      "This patch is 7th patch output\n",
      "tensor([380., 380., 380., 380., 380., 380., 380., 380., 380., 380., 380., 380.,\n",
      "        380., 380., 380., 380., 380., 380., 380., 380., 380., 380., 380., 380.,\n",
      "        380., 380., 380., 380., 380., 380., 380., 380.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This patch is 8th patch input\n",
      "tensor([8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.,\n",
      "        8., 8., 8., 8., 8., 8., 8., 8., 8.])\n",
      "This patch is 8th patch output\n",
      "tensor([434., 434., 434., 434., 434., 434., 434., 434., 434., 434., 434., 434.,\n",
      "        434., 434., 434., 434., 434., 434., 434., 434., 434., 434., 434., 434.,\n",
      "        434., 434., 434., 434., 434., 434., 434., 434.],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# check the output each patch\n",
    "for i in range(9):\n",
    "    # input data\n",
    "    print(f\"This patch is {i}th patch input\")\n",
    "    print(ps_output[0, i])\n",
    "    # output data\n",
    "    print(f\"This patch is {i}th patch output\")\n",
    "    print(pe_output[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare conv2d\n",
    "# conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=3)\n",
    "# for conv_parameter in conv.parameters():\n",
    "#     conv_parameter.data.fill_(2)\n",
    "# conv_output = conv(batch_debug_image)\n",
    "# conv_output = conv_output.flatten(start_dim=2).permute(0, 2, 1)\n",
    "\n",
    "# for i in range(9):\n",
    "#     print(pe_output[0, i])\n",
    "#     print(conv_output[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56,\n",
      "        56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56])\n",
      "tensor([110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n",
      "        110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n",
      "        110, 110, 110, 110])\n",
      "tensor([164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164,\n",
      "        164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164,\n",
      "        164, 164, 164, 164])\n",
      "tensor([218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218,\n",
      "        218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218,\n",
      "        218, 218, 218, 218])\n",
      "tensor([272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272,\n",
      "        272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272, 272,\n",
      "        272, 272, 272, 272])\n",
      "tensor([326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326,\n",
      "        326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326,\n",
      "        326, 326, 326, 326])\n",
      "tensor([380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380,\n",
      "        380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380, 380,\n",
      "        380, 380, 380, 380])\n",
      "tensor([434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434])\n"
     ]
    }
   ],
   "source": [
    "weight_fill_value = 2\n",
    "test_weights = torch.full((27, 32), weight_fill_value)\n",
    "test_bias = weight_fill_value\n",
    "for i in range(9):\n",
    "    test_input = torch.full((27,), i)\n",
    "    print(test_input @ test_weights + test_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Mixer Layer\n",
    "\n",
    "MixerLayer에서 Token과 Channel을 섞는 부분이 핵심이 되어 사용되는 벡터의 차원의 의미를 이해하고 구현해보세요.\n",
    "그리고 MixerLayer에는 skip-connect가 두 곳이 있습니다. 이를 유의하여 구현해보세요.\n",
    "(* skip-connect는 방정식으로 표현하면 `x + f(x)`가 됩니다.)\n",
    "\n",
    "- 제안 함수\n",
    "    - `torch.nn.Linear` : 주어진 input을 linear transformation합니다. args로는 `in_features`, `out_features`가 있습니다.\n",
    "    - `torch.nn.LayerNorm` : 주어진 shape에 대해 layer normalization을 수행합니다. args로는 `normalized_shape`, `eps`, `elementwise_affine`가 있습니다.\n",
    "    - `torch.nn.functional.gelu` : gelu activation function을 수행합니다.\n",
    "    - `Rearrange` : 주어진 차원을 순서대로 바꿉니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        # [START CODE]\n",
    "        self.linear_1 = nn.Linear(in_features, hidden_features)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear_2 = nn.Linear(hidden_features, out_features)\n",
    "        # [END CODE]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [START CODE]\n",
    "        x = self.linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "        # [END CODE]\n",
    "\n",
    "\n",
    "class MixerLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, n_patch):\n",
    "        super().__init__()\n",
    "        # [START CODE]\n",
    "        self.layer_norm_1 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.mlp_1 = nn.Sequential(\n",
    "            Rearrange(\"b n c -> b c n\"),\n",
    "            MLP(n_patch, hidden_dim, n_patch),\n",
    "            Rearrange(\"b c n -> b n c\"),\n",
    "        )\n",
    "\n",
    "        self.layer_norm_2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp_2 = MLP(emb_dim, hidden_dim, emb_dim)\n",
    "        # [END CODE]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [START CODE]\n",
    "        norm_x = self.layer_norm_1(x)\n",
    "        out_1 = self.mlp_1(norm_x) + x\n",
    "        norm_out_1 = self.layer_norm_2(out_1)\n",
    "        out_2 = self.mlp_2(norm_out_1)\n",
    "        out_2 = out_2 + out_1\n",
    "        return out_2\n",
    "        # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [CORRECT CODE]\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, in_features, hidden_features, out_features):\n",
    "#         super().__init__()\n",
    "#         # [START CODE]\n",
    "#         self.linear_1 = None\n",
    "#         self.activation = None\n",
    "#         self.linear_2 = None\n",
    "#         # [END CODE]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # [START CODE]\n",
    "#         return None\n",
    "#         # [END CODE]\n",
    "\n",
    "# class MixerLayer(nn.Module):\n",
    "#     def __init__(self, emb_dim, hidden_dim, n_patch):\n",
    "#         super().__init__()\n",
    "#         # [START CODE]\n",
    "#         self.layer_norm_1 = None\n",
    "\n",
    "#         self.mlp_1 = None\n",
    "\n",
    "#         self.layer_norm_2 = None\n",
    "#         self.mlp_2 = None\n",
    "#         # [END CODE]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # [START CODE]\n",
    "#         return None\n",
    "#         # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9, 32])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "hidden_dim = 64\n",
    "# [START CODE]\n",
    "n_patch = pe_output.shape[1]\n",
    "emb_dim = pe_output.shape[2]\n",
    "mixer_layer = MixerLayer(emb_dim=emb_dim, n_patch=n_patch, hidden_dim=hidden_dim)\n",
    "\n",
    "mixer_layer_output = mixer_layer(pe_output)\n",
    "# [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_dim = 64\n",
    "# # [START CODE]\n",
    "# n_patch = None\n",
    "# emb_dim = None\n",
    "# mixer_layer = MixerLayer(emb_dim=emb_dim, n_patch=n_patch, hidden_dim=hidden_dim)\n",
    "\n",
    "# mixer_layer_output = mixer_layer(pe_output)\n",
    "# # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Mixer\n",
    "앞서 만든 `PatchSplitter`, `PatchEmbedding`, `MixerLayer`를 이용하여 MLP-Mixer를 구현해보겠습니다. 이중 MixerLayer는 여러 개의 MixerLayer로 이루어질 수 있게 만들고 각 layer의 hidden_dim을 각각 다르게 설정할 수 있게 만들어보세요.\n",
    "\n",
    "- 제안 함수\n",
    "    - `torch.nn.ModuleList` : 주어진 모듈들을 리스트로 묶어주는 역할을 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CORRECT CODE]\n",
    "class MLPMixer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 32,\n",
    "        patch_size: int = 4,\n",
    "        emb_dim: int = 32,\n",
    "        hidden_dim: list = [64, 32, 16],\n",
    "        num_classes: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # [START CODE]\n",
    "        # check the image size is divisible by patch size\n",
    "        assert (\n",
    "            image_size % patch_size == 0\n",
    "        ), \"Image size must be divisible by patch size\"\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_splitter = PatchSplitter(patch_size, image_size, batch_size)\n",
    "\n",
    "        self.patch_emb = PatchEmbLayer(patch_size * patch_size * 3, emb_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for i, h_d in enumerate(hidden_dim):\n",
    "            self.layers.append(MixerLayer(emb_dim, h_d, self.num_patches))\n",
    "\n",
    "        self.output_layer = nn.Linear(emb_dim, num_classes)\n",
    "        # [END CODE]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [START CODE]\n",
    "        x = self.patch_splitter(x)\n",
    "        x = self.patch_emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "        # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLPMixer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         image_size: int = 32,\n",
    "#         patch_size: int = 4,\n",
    "#         emb_dim: int = 32,\n",
    "#         hidden_dim: list = [64, 32, 16],\n",
    "#         num_classes: int = 10,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         # [START CODE]\n",
    "#         # check the image size is divisible by patch size\n",
    "\n",
    "#         # assert 문을 사용하여 패치 크기로 나누어 떨어지지 않으면 에러를 발생시킵니다.\n",
    "#         assert None, \"Image size must be divisible by patch size\"\n",
    "#         self.num_patches = None\n",
    "\n",
    "#         self.patch_splitter = PatchSplitter(None, None, None)\n",
    "\n",
    "#         self.patch_emb = PatchEmbLayer(None, None)\n",
    "\n",
    "#         self.layers = nn.ModuleList([])\n",
    "\n",
    "#         # 각 MixerLayer에 서로 다른 hidden_dim을 self.layers에 추가합니다.\n",
    "\n",
    "\n",
    "#         self.output_layer = None\n",
    "#         # [END CODE]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # [START CODE]\n",
    "#         return None\n",
    "#         # [END CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "model = MLPMixer()\n",
    "sample_image = torch.randn(32, 3, 32, 32)\n",
    "output = model(sample_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate MLP-Mixer\n",
    "이제 구현한 MLP-Mixer를 이용하여 이미지를 분류할 수 있게 학습 코드를 만들어 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [START CODE]\n",
    "model = MLPMixer()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "do_train_test(model, train_loader, test_loader, criterion, optimizer, \"cuda\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(model, sample_image)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs (started 0:08:02 ago; pid 22344)\n"
     ]
    }
   ],
   "source": [
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir logs (started 0:07:15 ago; port 6006, pid 22344).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-451a803825adc78f\" width=\"100%\" height=\"1000\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-451a803825adc78f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
